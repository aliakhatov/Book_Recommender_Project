# MSiA423 BookHub Recommender

### Author: Alisher Akhatov

## Project Charter

![alt-text](figures/image_book.jpeg)

#### Vision

Since the invention of the printing press in 1440 by Johannes Gutenberg, 130+ million books have been published. According to [Bowker](https://www.bowker.com/siteassets/files/pdf-files/bowker-selfpublishing-report-2019.pdf), in 2018 alone, over 1.6 million titles were published in the United States. Another dramatic statistic comes from [mybookcave.com](https://mybookcave.medium.com/how-long-would-it-take-to-read-every-book-in-the-world-eaeab6e86a8c), which states that on average a person has to read 1,721,519 books a year to read every book in the world. However, [Pew Research Center](https://www.pewresearch.org/internet/2016/09/01/book-reading-2016/) claims that on average Americans tend to read close to 12 books per year. All of these mind-bending statistics demonstrate that citizens of the earth today face a sheer ton of struggles when picking their next book to read. To help them with this problem, our company came up with the idea of creating a personalized book recommender website. This website could be seen as a personalized assistant in discovering the next best books to read based on the person’s current favorite books. Sometimes, it is easier to let an algorithm pick one’s next book instead of reading through dozens of blogs and websites to make a choice.

#### Mission

There are several ways to achieve the goal of making a personalized book recommendation by using techniques such as k-means clustering, building recommender systems, etc. This project will be focused on building a specific technique of recommender systems - collaborative filtering. The data for this project - [Book-Crossing](http://www2.informatik.uni-freiburg.de/~cziegler/BX/), is sourced from Freiburg University’s website and it was collected by Cai-Nicolas Ziegler in 2004. 

##### Step by step guide for the website:

There are several steps that a user will have to undertake in order to get a personalized book recommendation. Once the user goes to the website, the 50 most popular books of all time sourced from New York Times will be presented to a user. The user will be prompted to click/select some of the books that he/she loves. Once the user selects specific books, the website will output a list of recommended books (up to 10 books). 

#### Success Criteria

There are up-stream and down-stream metrics that recommender systems could be evaluated on. 
For upstream metrics the model will be assessed on how accurately it can predict how much the user is going to like the book. 
The main accuracy metric that the model will be assessed by is precision and recall.
To find these metrics, I am holding out a portion of items, make recommendations based on the remaining set of items, 
then see how many of those recommendations the user actually read or rated the book.
These metrics have some drawbacks related to prediction diversity, 
prediction context and order of predictions. Despite those issues, they are still one of the most
commonly used measures to evaluate a collaborative filtering system. 

From a business perspective, in the medium term, the assessment of the recommender tool will be tied to the amount of revenue generated. 
The company would generate revenue every time a user buys a book using the link provided on the website. 
The ultimate down-stream metric will be the lifetime value of the user. 
The goal of the tool will be to help the company to increase Customer Lifetime Value (CLV) 
by turning users into repeat customers for personalized book recommendations.

## Directory structure 

```
├── README.md                         <- You are here
├── api
│   ├── static/                       <- CSS, JS files that remain static
│   ├── templates/                    <- HTML (or other code) that is templated and changes based on a set of inputs│    
│
├── config                            <- Directory for configuration files 
│   ├── local/                        <- Directory for keeping environment variables and other local configurations that *do not sync** to Github 
│   ├── logging/                      <- Configuration of python loggers
│   ├── flaskconfig.py                <- Configurations for Flask API 
│   ├── config.yaml                   <- Main configuration file that's used for model pipeline and web app   
│
├── data                              <- Folder that contains data used or generated. Only the external/ and sample/ subdirectories are tracked by git. 
│   ├── raw/                          <- Raw data sources downloaded from the public url
│   ├── processed/                    <- Data that was cleaned and generated by run_model.py
│   ├── results/                      <- Model metrics that were generated by `evaluate` and `recommend` stages of pipeline
│
├── deliverables/                     <- Any white papers, presentations, final work products that are presented or delivered to a stakeholder 
│
|
├── dockerfiles/                      <- Directory for all project-related Dockerfiles 
│   ├── Dockerfile.app                <- Dockerfile for building image to run web app
│   ├── Dockerfile                    <- Dockerfile for building image to execute run_model.py and run_acquire.py  
│   ├── Dockerfile.test               <- Dockerfile for building image to run unit tests
│
├── figures/                          <- Generated graphics and figures to be used in reporting, documentation, etc
│
├── models/                           <- Trained model objects (TMOs), model predictions, and/or model summaries
│
├── notebooks/
│   ├── archive/                      <- Develop notebooks no longer being used.
│   ├── deliver/                      <- Notebooks shared with others / in final state
│   ├── develop/                      <- Current notebooks being used in development. 
│
├── reference/                        <- Any reference material relevant to the project
│
├── src/                              <- Source data for the project. No executable Python files should live in this folder.  
│
├── tests/                             <- Files necessary for running model tests (see documentation below) 
│
├── app.py                            <- Flask wrapper for running the web app 
├── run_model.py                      <- Executes data cleaning, feature generation and model building and evaluation
├── run_acquire.py                    <- Executes raw data acquisition, uploading the data to s3 and database actions  
├── requirements.txt                  <- Python package dependencies 
```

## 1. Initialize the Project :green_circle:

### 1.1 Build the Book Recommender Image  

To build the image, run from this directory (the root of the repo): 

```bash
 make image
```

### 1.2 Download the data from the public [url](http://www2.informatik.uni-freiburg.de/~cziegler/BX/)  
This step performs 2 actions by default: 
1. downloading the raw files from the public url 
2. uploading downloaded files to s3.

If you do not want to upload the files to s3, specify it by 
an additional argument: `make acquire ARGS=--download_only`

In order to upload downloaded files to S3 Bucket, you have to first set 
AWS credentials as environment variables in your terminal. This step could be accomplished
by running following commands:

```bash
export AWS_ACCESS_KEY_ID="YOUR ACCESS KEY"
export AWS_SECRET_ACCESS_KEY="YOUR SECRET KEY"
````
or creating a file in `config/` directory and sourcing it. Current default s3 bucket is set to
`s3://2022-msia-423-akhatov-alisher/raw/`.

Once your added your AWS credentials, please run the following:
```bash
make acquire
````
 

## 2. Build a model :hammer:

### 2.1 Build a pipeline
If you followed step 1, now we can build a collaborative filtering model. Please take into account that
without following the first step you cannot run step 2.  
To make it user-friendly I created one-step process for executing all of the following steps:
1. Preprocess existing raw data
2. Generate features that are used in model training process 
3. Train the model
4. Recommend Books based on user selection
5. Evaluate the model

```bash
make full-pipeline
```

### 2.1 Step by step model execution
If you want to run all of these steps mentioned above individually, please follow the guidelines listed below.
At each step of the pipeline multiple files will be saved in `data/` folder.
For example, preprocessing step will generate cleaned dataset. In order to run just preprocessing step
please enter the following to your terminal:

```bash
make preprocess
```

Generate features step will take that cleaned data and create two files: matrix of users' ratings and top books. 
You can select number of books presented as top books to you by changing the parameter in yaml configuration file.
To run `generate_features` step please run:

```bash
make generate_features
```

Afterwards, train a model step will split the data set to test and train sets and train the Nearest Neighbors
model on the training data. Train dataframe, test dataframe and the model object is generated at this step.
To execute the training step please run:
```bash
make train
```
Everything up until this step will be saved in `data/processed/`. The further steps
wil be saved to `data/results/` directory. 

Now we can test the model by generating recommendations on some sample titles.
I picked the following sample titles (different set of titles could be picked 
in yaml configuration file):   
* The Little Prince
* To Kill a Mockingbird
* The Secret Garden

To generate recommendations on these titles please run:

```bash
make recommend
```

The last but not the least step is evaluation of the model. To evaluate the model 
based on precision and recall metrics please execute:

```bash
make evaluate
```
The results of `recommend` and `evaluate` steps are saved into `data/results/` directory.


## 3. Launch the App :electric_plug: 
### 3.1 RDS Credentials Setup
In order for the app to interact with your RDS database you have to configure environment variables 
by running either one of these options. Please replace values in `" "` with your credentials.
###### Option 1: 
```bash
export MYSQL_USER="MY_USERNAME"
export MYSQL_PASSWORD="MY_PASSWORD"
export MYSQL_HOST="MY_HOST"
export MYSQL_PORT="MY_PORT"
export MYSQL_DATABASE="MY_DATABASE"
```

###### Option 2:
```bash
export SQLALCHEMY_DATABASE_URI="YOUR_DATABASE_URI"
```
If you did not export your credentials as mentioned above, 
SQLite local database `data/book_recommender.db` will be created.

Now, after setting up all the database configurations, to create the database run:
```bash
make create_db
```

### 3.2 Ingest data
In order to ingest the data to the database (RDS or SQLite), please run:

```bash
make full_ingest
```
This command will ingest Top n (user's choice, default = 20) books to the database.

### 3.3 Build app image

Finally, we can build and use the app! Please make sure that you created the database
and ingested the database with the top books. If you ran everything as directed, now we can
build an app docker image `final-project-app`, by executing:
```bash
make app
```
### 3.4 Run the app

After successfully building the docker image, to run the app please run the 
following command:

```bash
make run-app
```
This command will create an instance of the app at `https://localhost:5002`.

**Note**: `SQLALCHEMY_DATABASE_URI` database configuration that you defined earlier will
determine whether the app will use local sqlite database or RDS on the cloud.

#### Kill the container 

Once finished with the app, you will need to kill the container. If you named the container, you can execute the following: 

```bash
docker kill final-project-app 
```
where `final-project-app` is the name given in the `docker run` command.

If you did not name the container, you can look up its name by running the following:

```bash 
docker container ls
```
The name will be provided in the right most column. 

## 4. Testing

For unit testing to build a docker image execute:

```bash
make tests
```

To run the tests, run: 

```bash
make run-tests
```

The following command will be executed within the container to run the provided unit tests under `tests/`:  

```bash
python -m pytest
```

# Thank you for looking at my repo! 